{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-04-04T07:09:30.556955Z",
     "iopub.status.busy": "2022-04-04T07:09:30.556677Z",
     "iopub.status.idle": "2022-04-04T07:09:33.819436Z",
     "shell.execute_reply": "2022-04-04T07:09:33.818642Z",
     "shell.execute_reply.started": "2022-04-04T07:09:30.556902Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from keras.layers import Input, Embedding, LSTM, TimeDistributed, Dense, Bidirectional\n",
    "from keras.models import Model, load_model\n",
    "\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9c5007896f6a9aaf3395fc82447ca2bb5cc57b65"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2022-04-04T07:09:33.820883Z",
     "iopub.status.busy": "2022-04-04T07:09:33.820585Z",
     "iopub.status.idle": "2022-04-04T07:09:34.543181Z",
     "shell.execute_reply": "2022-04-04T07:09:34.542420Z",
     "shell.execute_reply.started": "2022-04-04T07:09:33.820838Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "lines = open('../input/movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "conv_lines = open('../input/movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "3983a4469fd5ed2f4b8db5a6cc41eda070c9142f",
    "execution": {
     "iopub.execute_input": "2022-04-04T07:09:34.544787Z",
     "iopub.status.busy": "2022-04-04T07:09:34.544471Z",
     "iopub.status.idle": "2022-04-04T07:09:34.911348Z",
     "shell.execute_reply": "2022-04-04T07:09:34.910664Z",
     "shell.execute_reply.started": "2022-04-04T07:09:34.544744Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a dictionary to map each line's id with its text\n",
    "id2line = {}\n",
    "for line in lines:\n",
    "    _line = line.split(' +++$+++ ')\n",
    "    if len(_line) == 5:\n",
    "        id2line[_line[0]] = _line[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "260fe85783607a5f8190796b4415dee00cdeabc5",
    "execution": {
     "iopub.execute_input": "2022-04-04T07:09:34.912941Z",
     "iopub.status.busy": "2022-04-04T07:09:34.912664Z",
     "iopub.status.idle": "2022-04-04T07:09:35.247228Z",
     "shell.execute_reply": "2022-04-04T07:09:35.246517Z",
     "shell.execute_reply.started": "2022-04-04T07:09:34.912898Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a list of all of the conversations' lines' ids.\n",
    "convs = []\n",
    "for line in conv_lines[:-1]:\n",
    "    _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
    "    convs.append(_line.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "c7ac8d38b75b1101165c37cdab3fca42597eb4d7",
    "execution": {
     "iopub.execute_input": "2022-04-04T07:09:35.250726Z",
     "iopub.status.busy": "2022-04-04T07:09:35.250491Z",
     "iopub.status.idle": "2022-04-04T07:09:35.259483Z",
     "shell.execute_reply": "2022-04-04T07:09:35.258776Z",
     "shell.execute_reply.started": "2022-04-04T07:09:35.250682Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L3490 That's what he did to me.  He put cigarettes out on me.\n",
      "L3491 Your father put cigarettes out on you?\n",
      "L3492 Out on my back when I was a small boy.\n",
      "L3493 Can I see your back?\n"
     ]
    }
   ],
   "source": [
    "#id and conversation sample\n",
    "for k in convs[300]:\n",
    "    print (k, id2line[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "a3142e705c67fd80837a998a7884dd584c30f57c",
    "execution": {
     "iopub.execute_input": "2022-04-04T07:09:35.260969Z",
     "iopub.status.busy": "2022-04-04T07:09:35.260656Z",
     "iopub.status.idle": "2022-04-04T07:09:35.524154Z",
     "shell.execute_reply": "2022-04-04T07:09:35.523395Z",
     "shell.execute_reply.started": "2022-04-04T07:09:35.260924Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221616\n",
      "221616\n"
     ]
    }
   ],
   "source": [
    "# Sort the sentences into questions (inputs) and answers (targets)\n",
    "questions = []#input\n",
    "answers = []#target\n",
    "for conv in convs:\n",
    "    for i in range(len(conv)-1):\n",
    "        questions.append(id2line[conv[i]])\n",
    "        answers.append(id2line[conv[i+1]])\n",
    "        \n",
    "# Compare lengths of questions and answers\n",
    "print(len(questions))\n",
    "print(len(answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "85f3e1713f0620f066f5a5ee34b31c9a73d768fa",
    "execution": {
     "iopub.execute_input": "2022-04-04T07:09:35.525754Z",
     "iopub.status.busy": "2022-04-04T07:09:35.525326Z",
     "iopub.status.idle": "2022-04-04T07:09:35.581936Z",
     "shell.execute_reply": "2022-04-04T07:09:35.581188Z",
     "shell.execute_reply.started": "2022-04-04T07:09:35.525704Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "#     text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    text = \" \".join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "162b9f09f2e3e50ed92f157a5ce0faf3bb0d0019",
    "execution": {
     "iopub.execute_input": "2022-04-04T07:09:35.583609Z",
     "iopub.status.busy": "2022-04-04T07:09:35.583337Z",
     "iopub.status.idle": "2022-04-04T07:09:48.414358Z",
     "shell.execute_reply": "2022-04-04T07:09:48.413689Z",
     "shell.execute_reply.started": "2022-04-04T07:09:35.583565Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "clean_questions = []\n",
    "for question in questions:\n",
    "    clean_questions.append(clean_text(question))\n",
    "clean_answers = []    \n",
    "for answer in answers:\n",
    "    clean_answers.append(clean_text(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "b1df72b880bbf79a8043ae3b608d37e89b5807e9",
    "execution": {
     "iopub.execute_input": "2022-04-04T07:09:48.415841Z",
     "iopub.status.busy": "2022-04-04T07:09:48.415587Z",
     "iopub.status.idle": "2022-04-04T07:09:49.074607Z",
     "shell.execute_reply": "2022-04-04T07:09:49.073848Z",
     "shell.execute_reply.started": "2022-04-04T07:09:48.415800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.0\n",
      "19.0\n",
      "24.0\n",
      "32.0\n"
     ]
    }
   ],
   "source": [
    "# Find the length of sentences (not using nltk due to processing speed)\n",
    "lengths = []\n",
    "# lengths.append([len(nltk.word_tokenize(sent)) for sent in clean_questions]) #nltk approach\n",
    "for question in clean_questions:\n",
    "    lengths.append(len(question.split()))\n",
    "for answer in clean_answers:\n",
    "    lengths.append(len(answer.split()))\n",
    "# Create a dataframe so that the values can be inspected\n",
    "lengths = pd.DataFrame(lengths, columns=['counts'])\n",
    "print(np.percentile(lengths, 80))\n",
    "print(np.percentile(lengths, 85))\n",
    "print(np.percentile(lengths, 90))\n",
    "print(np.percentile(lengths, 95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "0035c49ba3da36e903a600b22c737e6a5c34de6e",
    "execution": {
     "iopub.execute_input": "2022-04-04T07:09:49.076420Z",
     "iopub.status.busy": "2022-04-04T07:09:49.075914Z",
     "iopub.status.idle": "2022-04-04T07:09:50.054429Z",
     "shell.execute_reply": "2022-04-04T07:09:50.053554Z",
     "shell.execute_reply.started": "2022-04-04T07:09:49.076371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138335\n",
      "138335\n"
     ]
    }
   ],
   "source": [
    "# Remove questions and answers that are shorter than 1 word and longer than 20 words.\n",
    "min_line_length = 2\n",
    "max_line_length = 20\n",
    "\n",
    "# Filter out the questions that are too short/long\n",
    "short_questions_temp = []\n",
    "short_answers_temp = []\n",
    "\n",
    "for i, question in enumerate(clean_questions):\n",
    "    if len(question.split()) >= min_line_length and len(question.split()) <= max_line_length:\n",
    "        short_questions_temp.append(question)\n",
    "        short_answers_temp.append(clean_answers[i])\n",
    "\n",
    "# Filter out the answers that are too short/long\n",
    "short_questions = []\n",
    "short_answers = []\n",
    "\n",
    "for i, answer in enumerate(short_answers_temp):\n",
    "    if len(answer.split()) >= min_line_length and len(answer.split()) <= max_line_length:\n",
    "        short_answers.append(answer)\n",
    "        short_questions.append(short_questions_temp[i])\n",
    "        \n",
    "print(len(short_questions))\n",
    "print(len(short_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "85389732fd564d7b160339bb26d0d1b5dea41fe7",
    "execution": {
     "iopub.execute_input": "2022-04-04T07:09:50.056231Z",
     "iopub.status.busy": "2022-04-04T07:09:50.055705Z",
     "iopub.status.idle": "2022-04-04T07:09:50.068902Z",
     "shell.execute_reply": "2022-04-04T07:09:50.067900Z",
     "shell.execute_reply.started": "2022-04-04T07:09:50.056171Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is war the weakest suffer the most\n",
      "you are american\n",
      "\n",
      "you are american\n",
      "so are you\n",
      "\n",
      "mr creasy i wanted to make sure you have everything you need\n",
      "i am fine\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = np.random.randint(1,len(short_questions))\n",
    "\n",
    "for i in range(r, r+3):\n",
    "    print(short_questions[i])\n",
    "    print(short_answers[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5825a5967e5a04f13b0a755e8ba304c419af4b33"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "106248b9b0ac7499c2b34cc3ae1c20a2e0ea41c4",
    "execution": {
     "iopub.execute_input": "2022-04-04T07:09:50.070783Z",
     "iopub.status.busy": "2022-04-04T07:09:50.070338Z",
     "iopub.status.idle": "2022-04-04T07:09:59.034102Z",
     "shell.execute_reply": "2022-04-04T07:09:59.033442Z",
     "shell.execute_reply.started": "2022-04-04T07:09:50.070607Z"
    }
   },
   "outputs": [],
   "source": [
    "#choosing number of samples\n",
    "num_samples = 30000  # Number of samples to train on.\n",
    "short_questions = short_questions[:num_samples]\n",
    "short_answers = short_answers[:num_samples]\n",
    "#tokenizing the qns and answers\n",
    "short_questions_tok = [nltk.word_tokenize(sent) for sent in short_questions]\n",
    "short_answers_tok = [nltk.word_tokenize(sent) for sent in short_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "51918567f4f43ae13a8d58a00bda15b1d508b191",
    "execution": {
     "iopub.execute_input": "2022-04-04T07:09:59.035899Z",
     "iopub.status.busy": "2022-04-04T07:09:59.035572Z",
     "iopub.status.idle": "2022-04-04T07:09:59.204293Z",
     "shell.execute_reply": "2022-04-04T07:09:59.203630Z",
     "shell.execute_reply.started": "2022-04-04T07:09:59.035857Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a dictionary for the frequency of the vocabulary\n",
    "# Create \n",
    "vocab = {}\n",
    "for question in short_questions_tok:\n",
    "    for word in question:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1\n",
    "\n",
    "for answer in short_answers_tok:\n",
    "    for word in answer:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "30d25056e8f62453bcde0a16c8a7933275f431c9",
    "execution": {
     "iopub.execute_input": "2022-04-04T07:09:59.206055Z",
     "iopub.status.busy": "2022-04-04T07:09:59.205610Z",
     "iopub.status.idle": "2022-04-04T07:09:59.216510Z",
     "shell.execute_reply": "2022-04-04T07:09:59.215890Z",
     "shell.execute_reply.started": "2022-04-04T07:09:59.205916Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove rare words from the vocabulary.\n",
    "# We will aim to replace fewer than 5% of words with <UNK>\n",
    "# You will see this ratio soon.\n",
    "threshold = 15\n",
    "count = 0\n",
    "for k,v in vocab.items():\n",
    "    if v >= threshold:\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "4c980b035173d12d2bf06dafd6410eb5da6023b2",
    "execution": {
     "iopub.execute_input": "2022-04-04T07:09:59.218027Z",
     "iopub.status.busy": "2022-04-04T07:09:59.217658Z",
     "iopub.status.idle": "2022-04-04T07:09:59.224463Z",
     "shell.execute_reply": "2022-04-04T07:09:59.223711Z",
     "shell.execute_reply.started": "2022-04-04T07:09:59.217961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of total vocab: 16746\n",
      "Size of vocab we will use: 1931\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of total vocab:\", len(vocab))\n",
    "print(\"Size of vocab we will use:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "d62a629773e34bb7bc1a12508cce820c0e47962d",
    "execution": {
     "iopub.execute_input": "2022-04-04T07:09:59.226253Z",
     "iopub.status.busy": "2022-04-04T07:09:59.225757Z",
     "iopub.status.idle": "2022-04-04T07:09:59.243958Z",
     "shell.execute_reply": "2022-04-04T07:09:59.243162Z",
     "shell.execute_reply.started": "2022-04-04T07:09:59.226206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of vocab used: 1933\n"
     ]
    }
   ],
   "source": [
    "#we will create dictionaries to provide a unique integer for each word.\n",
    "WORD_CODE_START = 1\n",
    "WORD_CODE_PADDING = 0\n",
    "\n",
    "\n",
    "word_num  = 2 #number 1 is left for WORD_CODE_START for model decoder later\n",
    "encoding = {}\n",
    "decoding = {1: 'START'}\n",
    "for word, count in vocab.items():\n",
    "    if count >= threshold: #get vocabularies that appear above threshold count\n",
    "        encoding[word] = word_num \n",
    "        decoding[word_num ] = word\n",
    "        word_num += 1\n",
    "\n",
    "print(\"No. of vocab used:\", word_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "de2e46d3826def6b6c94a9827c32118d555fae2e",
    "execution": {
     "iopub.execute_input": "2022-04-04T07:09:59.245727Z",
     "iopub.status.busy": "2022-04-04T07:09:59.245379Z",
     "iopub.status.idle": "2022-04-04T07:09:59.251951Z",
     "shell.execute_reply": "2022-04-04T07:09:59.251335Z",
     "shell.execute_reply.started": "2022-04-04T07:09:59.245670Z"
    }
   },
   "outputs": [],
   "source": [
    "#include unknown token for words not in dictionary\n",
    "decoding[len(encoding)+2] = '<UNK>'\n",
    "encoding['<UNK>'] = len(encoding)+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "bccd51e8730e7c3da1234af9480da767f400c230",
    "execution": {
     "iopub.execute_input": "2022-04-04T07:09:59.254814Z",
     "iopub.status.busy": "2022-04-04T07:09:59.254587Z",
     "iopub.status.idle": "2022-04-04T07:09:59.263993Z",
     "shell.execute_reply": "2022-04-04T07:09:59.263372Z",
     "shell.execute_reply.started": "2022-04-04T07:09:59.254767Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1934"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_size = word_num+1\n",
    "dict_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7ea3c32a6180ee8fea8b1490b40351897cc426e6"
   },
   "source": [
    "# Vectorizing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "4de33f91476bd6c50cf7c9322f273079783ceec2",
    "execution": {
     "iopub.execute_input": "2022-04-04T07:09:59.265609Z",
     "iopub.status.busy": "2022-04-04T07:09:59.265229Z",
     "iopub.status.idle": "2022-04-04T07:09:59.280023Z",
     "shell.execute_reply": "2022-04-04T07:09:59.279194Z",
     "shell.execute_reply.started": "2022-04-04T07:09:59.265547Z"
    }
   },
   "outputs": [],
   "source": [
    "def transform(encoding, data, vector_size=20):\n",
    "    \"\"\"\n",
    "    :param encoding: encoding dict built by build_word_encoding()\n",
    "    :param data: list of strings\n",
    "    :param vector_size: size of each encoded vector\n",
    "    \"\"\"\n",
    "    transformed_data = np.zeros(shape=(len(data), vector_size))\n",
    "    for i in range(len(data)):\n",
    "        for j in range(min(len(data[i]), vector_size)):\n",
    "            try:\n",
    "                transformed_data[i][j] = encoding[data[i][j]]\n",
    "            except:\n",
    "                transformed_data[i][j] = encoding['<UNK>']\n",
    "    return transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T07:09:59.281554Z",
     "iopub.status.busy": "2022-04-04T07:09:59.281263Z",
     "iopub.status.idle": "2022-04-04T07:09:59.328428Z",
     "shell.execute_reply": "2022-04-04T07:09:59.327484Z",
     "shell.execute_reply.started": "2022-04-04T07:09:59.281488Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size 24000\n",
      "validation size 5400\n",
      "testing size 600\n"
     ]
    }
   ],
   "source": [
    "#train-validation split\n",
    "data_size = len(short_questions_tok)\n",
    "\n",
    "# We will use the first 0-80th %-tile (80%) of data for the training\n",
    "training_input  = short_questions_tok[:round(data_size*(80/100))]\n",
    "training_input  = [tr_input[::-1] for tr_input in training_input] #reverseing input seq for better performance\n",
    "training_output = short_answers_tok[:round(data_size*(80/100))]\n",
    "\n",
    "# We will use the remaining for validation\n",
    "validation_input = short_questions_tok[round(data_size*(80/100)):round(data_size*(98/100))]\n",
    "validation_input  = [val_input[::-1] for val_input in validation_input] #reverseing input seq for better performance\n",
    "validation_output = short_answers_tok[round(data_size*(80/100)):round(data_size*(98/100))]\n",
    "\n",
    "#we will use the remaining for testing\n",
    "testing_input = short_questions_tok[-round(data_size*(2/100)):]\n",
    "testing_input  = [val_input[::-1] for val_input in testing_input] #reverseing input seq for better performance\n",
    "testing_output = short_answers_tok[-round(data_size*(2/100)):]\n",
    "\n",
    "print('training size', len(training_input))\n",
    "print('validation size', len(validation_input))\n",
    "print('testing size', len(testing_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "f13007fbe8661c4c2ea4c35fd51b86c4fc220efb",
    "execution": {
     "iopub.execute_input": "2022-04-04T07:09:59.330241Z",
     "iopub.status.busy": "2022-04-04T07:09:59.329713Z",
     "iopub.status.idle": "2022-04-04T07:09:59.637968Z",
     "shell.execute_reply": "2022-04-04T07:09:59.637215Z",
     "shell.execute_reply.started": "2022-04-04T07:09:59.330189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_training_input (24000, 20)\n",
      "encoded_training_output (24000, 20)\n"
     ]
    }
   ],
   "source": [
    "INPUT_LENGTH = 20\n",
    "OUTPUT_LENGTH = 20\n",
    "#encoding training set\n",
    "encoded_training_input = transform(\n",
    "    encoding, training_input, vector_size=INPUT_LENGTH)\n",
    "encoded_training_output = transform(\n",
    "    encoding, training_output, vector_size=OUTPUT_LENGTH)\n",
    "\n",
    "print('encoded_training_input', encoded_training_input.shape)\n",
    "print('encoded_training_output', encoded_training_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "cbbf370a5d0640a4dea6b6d9237a5917978b92a8",
    "execution": {
     "iopub.execute_input": "2022-04-04T07:09:59.639551Z",
     "iopub.status.busy": "2022-04-04T07:09:59.639117Z",
     "iopub.status.idle": "2022-04-04T07:09:59.719771Z",
     "shell.execute_reply": "2022-04-04T07:09:59.719076Z",
     "shell.execute_reply.started": "2022-04-04T07:09:59.639501Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_validation_input (5400, 20)\n",
      "encoded_validation_output (5400, 20)\n"
     ]
    }
   ],
   "source": [
    "#encoding validation set\n",
    "encoded_validation_input = transform(\n",
    "    encoding, validation_input, vector_size=INPUT_LENGTH)\n",
    "encoded_validation_output = transform(\n",
    "    encoding, validation_output, vector_size=OUTPUT_LENGTH)\n",
    "\n",
    "print('encoded_validation_input', encoded_validation_input.shape)\n",
    "print('encoded_validation_output', encoded_validation_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T07:09:59.721379Z",
     "iopub.status.busy": "2022-04-04T07:09:59.720977Z",
     "iopub.status.idle": "2022-04-04T07:09:59.737441Z",
     "shell.execute_reply": "2022-04-04T07:09:59.736651Z",
     "shell.execute_reply.started": "2022-04-04T07:09:59.721254Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_testing_input (600, 20)\n",
      "encoded_testing_output (600, 20)\n"
     ]
    }
   ],
   "source": [
    "#encoding testing set\n",
    "encoded_testing_input = transform(\n",
    "    encoding, testing_input, vector_size=INPUT_LENGTH)\n",
    "encoded_testing_output = transform(\n",
    "    encoding, testing_output, vector_size=OUTPUT_LENGTH)\n",
    "\n",
    "print('encoded_testing_input', encoded_testing_input.shape)\n",
    "print('encoded_testing_output', encoded_testing_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T07:09:59.739182Z",
     "iopub.status.busy": "2022-04-04T07:09:59.738695Z",
     "iopub.status.idle": "2022-04-04T07:10:04.928406Z",
     "shell.execute_reply": "2022-04-04T07:10:04.927686Z",
     "shell.execute_reply.started": "2022-04-04T07:09:59.739120Z"
    }
   },
   "outputs": [],
   "source": [
    "training_encoder_input = encoded_training_input\n",
    "training_decoder_input = np.zeros_like(encoded_training_output)\n",
    "training_decoder_input[:, 1:] = encoded_training_output[:,:-1]\n",
    "training_decoder_input[:, 0] = WORD_CODE_START\n",
    "training_decoder_output = np.eye(dict_size)[encoded_training_output.astype('int')]\n",
    "\n",
    "validation_encoder_input = encoded_validation_input\n",
    "validation_decoder_input = np.zeros_like(encoded_validation_output)\n",
    "validation_decoder_input[:, 1:] = encoded_validation_output[:,:-1]\n",
    "validation_decoder_input[:, 0] = WORD_CODE_START\n",
    "validation_decoder_output = np.eye(dict_size)[encoded_validation_output.astype('int')]\n",
    "\n",
    "testing_encoder_input = encoded_testing_input\n",
    "testing_decoder_input = np.zeros_like(encoded_testing_output)\n",
    "testing_decoder_input[:, 1:] = encoded_testing_output[:,:-1]\n",
    "testing_decoder_input[:, 0] = WORD_CODE_START\n",
    "testing_decoder_output = np.eye(dict_size)[encoded_testing_output.astype('int')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T07:13:17.735639Z",
     "iopub.status.busy": "2022-04-04T07:13:17.735362Z",
     "iopub.status.idle": "2022-04-04T07:13:17.743107Z",
     "shell.execute_reply": "2022-04-04T07:13:17.742264Z",
     "shell.execute_reply.started": "2022-04-04T07:13:17.735589Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5400, 20, 1934)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_decoder_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b52f0368fecf8ac79903927799fe67c104d8e518"
   },
   "source": [
    "# Attention Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "e8b5afc77468e4e263b3fc7ff62a2e6952a80e97",
    "execution": {
     "iopub.execute_input": "2022-04-04T07:10:04.929943Z",
     "iopub.status.busy": "2022-04-04T07:10:04.929583Z",
     "iopub.status.idle": "2022-04-04T07:10:04.953225Z",
     "shell.execute_reply": "2022-04-04T07:10:04.952656Z",
     "shell.execute_reply.started": "2022-04-04T07:10:04.929882Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "306b1fb012febd74dd40840c0588bdd8503ba65b",
    "execution": {
     "iopub.execute_input": "2022-04-04T07:10:04.955028Z",
     "iopub.status.busy": "2022-04-04T07:10:04.954568Z",
     "iopub.status.idle": "2022-04-04T07:10:04.960878Z",
     "shell.execute_reply": "2022-04-04T07:10:04.960232Z",
     "shell.execute_reply.started": "2022-04-04T07:10:04.954965Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_input = Input(shape=(INPUT_LENGTH,))\n",
    "decoder_input = Input(shape=(OUTPUT_LENGTH,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "d91367ec6c706059c4b520011fdec6e1051db38d",
    "execution": {
     "iopub.execute_input": "2022-04-04T07:13:40.257284Z",
     "iopub.status.busy": "2022-04-04T07:13:40.256948Z",
     "iopub.status.idle": "2022-04-04T07:13:42.772783Z",
     "shell.execute_reply": "2022-04-04T07:13:42.772141Z",
     "shell.execute_reply.started": "2022-04-04T07:13:40.257231Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import SimpleRNN\n",
    "\n",
    "encoder = Embedding(dict_size, 128, input_length=INPUT_LENGTH, mask_zero=True)(encoder_input)\n",
    "encoder = LSTM(512, return_sequences=True, unroll=True)(encoder)\n",
    "encoder_last = encoder[:,-1,:]\n",
    "\n",
    "decoder = Embedding(dict_size, 128, input_length=OUTPUT_LENGTH, mask_zero=True)(decoder_input)\n",
    "decoder = LSTM(512, return_sequences=True, unroll=True)(decoder, initial_state=[encoder_last, encoder_last])\n",
    "\n",
    "# For the plain Sequence-to-Sequence, we produced the output from directly from decoder\n",
    "# output = TimeDistributed(Dense(output_dict_size, activation=\"softmax\"))(decoder)\n",
    "\n",
    "from keras.layers import Activation, dot, concatenate\n",
    "\n",
    "attention = dot([decoder, encoder], axes=[2, 2])\n",
    "attention = Activation('softmax', name='attention')(attention)\n",
    "\n",
    "context = dot([attention, encoder], axes=[2,1])\n",
    "\n",
    "decoder_combined_context = concatenate([context, decoder])\n",
    "\n",
    "# Has another weight + tanh layer as described in equation (5) of the paper\n",
    "output = TimeDistributed(Dense(1934, activation=\"tanh\"))(decoder_combined_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "257dcf12770f8928f91754044d3efc5aff1ec296",
    "execution": {
     "iopub.execute_input": "2022-04-04T07:13:46.135968Z",
     "iopub.status.busy": "2022-04-04T07:13:46.135664Z",
     "iopub.status.idle": "2022-04-04T07:13:46.188875Z",
     "shell.execute_reply": "2022-04-04T07:13:46.188128Z",
     "shell.execute_reply.started": "2022-04-04T07:13:46.135914Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 20, 128)      247552      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 20, 128)      247552      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 20, 512)      1312768     embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 20, 512)      1312768     embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 20, 20)       0           lstm_2[0][0]                     \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention (Activation)          (None, 20, 20)       0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 20, 512)      0           attention[0][0]                  \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 20, 1024)     0           dot_2[0][0]                      \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 20, 1934)     1982350     concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 5,102,990\n",
      "Trainable params: 5,102,990\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[encoder_input, decoder_input], outputs=[output])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy',metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "3d43e174878dfbef64900b7f043d7bb41f458309",
    "execution": {
     "iopub.execute_input": "2022-04-04T07:13:50.204952Z",
     "iopub.status.busy": "2022-04-04T07:13:50.204678Z",
     "iopub.status.idle": "2022-04-04T07:14:17.143906Z",
     "shell.execute_reply": "2022-04-04T07:14:17.142616Z",
     "shell.execute_reply.started": "2022-04-04T07:13:50.204901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24000 samples, validate on 5400 samples\n",
      "Epoch 1/100\n",
      "24000/24000 [==============================] - 40s 2ms/step - loss: 0.0048 - acc: 0.9993 - val_loss: 0.0073 - val_acc: 0.9932\n",
      "Epoch 2/100\n",
      "24000/24000 [==============================] - 26s 1ms/step - loss: 0.0051 - acc: 0.9990 - val_loss: 0.0047 - val_acc: 0.9995\n",
      "Epoch 3/100\n",
      "24000/24000 [==============================] - 26s 1ms/step - loss: 0.0046 - acc: 0.9995 - val_loss: 0.0047 - val_acc: 0.9995\n",
      "Epoch 4/100\n",
      "24000/24000 [==============================] - 26s 1ms/step - loss: 0.0050 - acc: 0.9994 - val_loss: 0.0056 - val_acc: 0.9993\n",
      "Epoch 5/100\n",
      "24000/24000 [==============================] - 26s 1ms/step - loss: 0.0055 - acc: 0.9994 - val_loss: 0.0055 - val_acc: 0.9995\n",
      "Epoch 6/100\n",
      "24000/24000 [==============================] - 25s 1ms/step - loss: 0.0056 - acc: 0.9991 - val_loss: 0.0064 - val_acc: 0.9913\n",
      "Epoch 7/100\n",
      "24000/24000 [==============================] - 26s 1ms/step - loss: 0.0057 - acc: 0.9975 - val_loss: 0.0057 - val_acc: 0.9931\n",
      "Epoch 8/100\n",
      "24000/24000 [==============================] - 26s 1ms/step - loss: 0.0056 - acc: 0.9987 - val_loss: 0.0055 - val_acc: 0.9993\n",
      "Epoch 9/100\n",
      "24000/24000 [==============================] - 26s 1ms/step - loss: 0.0052 - acc: 0.9993 - val_loss: 0.0053 - val_acc: 0.9993\n",
      "Epoch 10/100\n",
      "24000/24000 [==============================] - 26s 1ms/step - loss: 0.0055 - acc: 0.9990 - val_loss: 0.0053 - val_acc: 0.9994\n",
      "Epoch 11/100\n",
      "24000/24000 [==============================] - 26s 1ms/step - loss: 0.0053 - acc: 0.9991 - val_loss: 0.0054 - val_acc: 0.9991\n",
      "Epoch 12/100\n",
      "24000/24000 [==============================] - 26s 1ms/step - loss: 0.0051 - acc: 0.9994 - val_loss: 0.0052 - val_acc: 0.9994\n",
      "Epoch 13/100\n",
      "24000/24000 [==============================] - 26s 1ms/step - loss: 0.0049 - acc: 0.9995 - val_loss: 0.0051 - val_acc: 0.9994\n",
      "Epoch 14/100\n",
      "  832/24000 [>.............................] - ETA: 21s - loss: 0.0049 - acc: 0.9994"
     ]
    }
   ],
   "source": [
    "history=model.fit(x=[training_encoder_input, training_decoder_input], y=[training_decoder_output],\n",
    "          validation_data=([validation_encoder_input, validation_decoder_input], [validation_decoder_output]),\n",
    "          #validation_split=0.05,\n",
    "          batch_size=64, epochs=100)\n",
    "\n",
    "model.save('model_attention.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "For testing purpose, we use 2% last data that data did not used in training or in validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "d0e3217f3d250c253dd457f192f54f8cd67c630f",
    "execution": {
     "iopub.status.busy": "2022-04-04T07:10:07.766700Z",
     "iopub.status.idle": "2022-04-04T07:10:07.767319Z"
    }
   },
   "outputs": [],
   "source": [
    "def prediction(raw_input):\n",
    "    clean_input = clean_text(raw_input)\n",
    "    input_tok = [nltk.word_tokenize(clean_input)]\n",
    "    input_tok = [input_tok[0][::-1]]  #reverseing input seq\n",
    "    encoder_input = transform(encoding, input_tok, 20)\n",
    "    decoder_input = np.zeros(shape=(len(encoder_input), OUTPUT_LENGTH))\n",
    "    decoder_input[:,0] = WORD_CODE_START\n",
    "    for i in range(1, OUTPUT_LENGTH):\n",
    "        output = model.predict([encoder_input, decoder_input]).argmax(axis=2)\n",
    "        decoder_input[:,i] = output[:,i]\n",
    "    return output\n",
    "\n",
    "def decode(decoding, vector):\n",
    "    \"\"\"\n",
    "    :param decoding: decoding dict built by word encoding\n",
    "    :param vector: an encoded vector\n",
    "    \"\"\"\n",
    "    text = ''\n",
    "    for i in vector:\n",
    "        if i == 0:\n",
    "            break\n",
    "        text += ' '\n",
    "        text += decoding[i]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "16e6ece57e4c02321c58176e5471f37ad1170837",
    "execution": {
     "iopub.status.busy": "2022-04-04T07:10:07.768202Z",
     "iopub.status.idle": "2022-04-04T07:10:07.768844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: did not you find out anything about the porters\n",
      "A:  i am right\n",
      "Q: she is worried about you and quite frankly so am i\n",
      "A:  i am not\n",
      "Q: noon exactly\n",
      "A:  i am right\n",
      "Q: kay that is your 20\n",
      "A:  <UNK> in the <UNK>\n",
      "Q: the high lama is the only one from whom any information can come\n",
      "A:  <UNK> <UNK>\n",
      "Q: i am just saying it was cold i think she kind of liked me\n",
      "A:  i am right\n",
      "Q: what are we going to do\n",
      "A:  i am right\n",
      "Q: cannot you see she just wanted her little girl back\n",
      "A:  i am right\n",
      "Q: by the way what religion do you follow here\n",
      "A:  <UNK> <UNK>\n",
      "Q: i am tired of owing you things you are free to go go ahead\n",
      "A:  i am right\n",
      "Q: i see you are afraid of going to jail eh\n",
      "A:  i am right\n",
      "Q: why what are you gonna do\n",
      "A:  i am right\n",
      "Q: john what is it\n",
      "A:  you are right\n",
      "Q: okay well i will see ya then\n",
      "A:  i am right\n",
      "Q: what if danny witwer came to you right now and insisted on a full chem run\n",
      "A:  i am right\n",
      "Q: he is here to help\n",
      "A:  i am right\n",
      "Q: i am fine\n",
      "A:  i am right\n",
      "Q: yes bob\n",
      "A:  i am right\n",
      "Q: hiding oh no hunting i was in the interior hunting fossils this morning i looked up suddenly\n",
      "A:  you mean you in the <UNK> in the <UNK> in the <UNK> in the <UNK>\n",
      "Q: as far away as i ever want to be\n",
      "A:  i am right\n"
     ]
    }
   ],
   "source": [
    "testing_input=short_questions[-round(data_size*(2/100)):]\n",
    "for i in range(20):\n",
    "    seq_index = np.random.randint(1, len(testing_input))\n",
    "    output = prediction(testing_input[seq_index])\n",
    "    print ('Q:', testing_input[seq_index])\n",
    "    print ('A:', decode(decoding, output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "c8909443bde5f6195e9a2983cdc633b9081c893d"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
