{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "from numpy import argmax\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file):\n",
    "    data=[]\n",
    "    with open (file) as read:\n",
    "        reader=csv.reader(read)\n",
    "        for row in reader:\n",
    "            data.append(row)\n",
    "    data=np.array(data)\n",
    "    data=data[:,-2:]\n",
    "    return data\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer(char_level=False)\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer\n",
    "\n",
    "# max sentence length\n",
    "def max_length(lines):\n",
    "\treturn max(len(line.split()) for line in lines)\n",
    "\n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == integer:\n",
    "\t\t\treturn word\n",
    "\treturn None\n",
    "\n",
    "# generate target given source sequence\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "    global prediksi\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    integers = [argmax(vector) for vector in prediction]\n",
    "    prediksi = prediction\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = word_for_id(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "        \n",
    "    return ' '.join(target)\n",
    "\n",
    "# translate\n",
    "def translate(model, all_tokenizer, sources):\n",
    "    for i, source in enumerate(sources):\n",
    "        # translate encoded source text\n",
    "        source = source.reshape((1, source.shape[0]))\n",
    "        translation = predict_sequence(model, all_tokenizer, source)\n",
    "    return translation\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "    \n",
    "def f1(y_true, y_pred):\n",
    "    result_precision = precision(y_true, y_pred)\n",
    "    result_recall = recall(y_true, y_pred)\n",
    "    return 2*((result_precision*result_recall)/(result_precision+result_recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "dataset=load_dataset('cleaning_data.csv')\n",
    "dataset=np.reshape(dataset,(-1,2))\n",
    "token_dataset=dataset.reshape(-1,1)#dataset to tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing\n",
    "tokenizer=create_tokenizer(token_dataset[:,0])\n",
    "wordindex=tokenizer.word_index\n",
    "len_vocab=len(wordindex)+1\n",
    "maxlength=max_length(token_dataset[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model('Model/model_gru_gru.h5', custom_objects={'precision':precision, 'recall':recall, 'f1':f1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'well i thought we would start with pronunciation if that is okay with you'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:,0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    short_questions=dataset[:,0]\n",
    "    seq_index = np.random.randint(1, len(short_questions))\n",
    "    q=short_questions[seq_index]\n",
    "    #we tokenize\n",
    "    X = tokenizer.texts_to_sequences(q)\n",
    "    X = pad_sequences(X, maxlength, padding='post')\n",
    "        \n",
    "    # find reply and print it out\n",
    "    a = translate(model, tokenizer, X)\n",
    "    #a = set(a)\n",
    "    words = a.split()\n",
    "    #print('ANSWER: %s' % (thing))\n",
    "    output= \" \".join(sorted(set(words), key=words.index))\n",
    "    print ('Q:', short_questions[seq_index])\n",
    "    print ('A:', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER: have do you\n",
      "ANSWER: have anything to you\n",
      "ANSWER: all respect you to\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    q = (input(str(\"YOU: \")))\n",
    "    if q == 'bye':\n",
    "        break\n",
    "    q = q.strip().split('\\n')\n",
    "\n",
    "    #we tokenize\n",
    "    X = tokenizer.texts_to_sequences(q)\n",
    "    X = pad_sequences(X, maxlength, padding='post')\n",
    "        \n",
    "    # find reply and print it out\n",
    "    a = translate(model, tokenizer, X)\n",
    "    #a = set(a)\n",
    "    words = a.split()\n",
    "    #print('ANSWER: %s' % (thing))\n",
    "    print ('ANSWER: ' + \" \".join(sorted(set(words), key=words.index)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96469a127def15a62ded042fba5089068494d4d23c4c88339d0c2940f4c47e25"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
